{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60978df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "from cv2 import resize\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e42ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007ed0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mild_heart_images = os.listdir(image_directory + 'Mild/')\n",
    "Moderate_heart_images = os.listdir(image_directory + 'Moderate/')\n",
    "No_heart_images = os.listdir(image_directory + 'No/')\n",
    "Proliferate_heart_images = os.listdir(image_directory + 'Proliferate/')\n",
    "Severe_heart_images = os.listdir(image_directory + 'Severe/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "256bcf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61cadeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6021306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , image_name in enumerate(No_heart_images):\n",
    "    if(image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'No/'+ image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6c46247",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , image_name in enumerate(Mild_heart_images):\n",
    "    if(image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Mild/'+ image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df1e888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , image_name in enumerate(Moderate_heart_images):\n",
    "    if(image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Moderate/'+ image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "633ca607",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , image_name in enumerate(Severe_heart_images):\n",
    "    if(image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Severe/'+ image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6582006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , image_name in enumerate(Proliferate_heart_images):\n",
    "    if(image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Proliferate/'+ image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9002981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3763\n",
      "3763\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(len(label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e409eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=np.array(dataset)\n",
    "label=np.array(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7a9d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = train_test_split(dataset, label, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "930d7c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">244</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">244</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">244</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">244</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">244</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">244</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,746,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,565</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m244\u001b[0m, \u001b[38;5;34m244\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m244\u001b[0m, \u001b[38;5;34m244\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m244\u001b[0m, \u001b[38;5;34m244\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m14,746,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │         \u001b[38;5;34m2,565\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,990,917</span> (57.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,990,917\u001b[0m (57.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,990,213</span> (57.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,990,213\u001b[0m (57.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> (2.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m704\u001b[0m (2.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "    layers.RandomFlip(mode=\"horizontal\"),\n",
    "])\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(244, 244, 3)),\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e889d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique label values in corrected test_labels: [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "valid_indices = np.where(test_labels < 5)[0]\n",
    "filtered_test_images = test_images[valid_indices]\n",
    "filtered_test_labels = test_labels[valid_indices]\n",
    "\n",
    "print(\"Unique label values in corrected test_labels:\", np.unique(filtered_test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72883302",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f78244e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, (244, 244))\n",
    "    image = image / 255.0  \n",
    "    return image, label\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((filtered_test_images, filtered_test_labels))\n",
    "test_dataset = test_dataset.map(preprocess).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b69b5a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.0684 - loss: 1.6330\n",
      "Test Loss: 1.6314523220062256\n",
      "Test Accuracy: 0.08499336242675781\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5df2a7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 8.499336242675781\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy:', test_acc*100 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e234877",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_without_optimizer.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee109ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.idea', 'dataset', 'heart_attack_prediction_model.h5', 'Images', 'main.py', 'my_model.h5', 'my_model.keras', 'my_model_without_optimizer.keras', 'New Text Document.txt', 'Prediction.ipynb', 'README.md', 'requirements.txt', 'static', 'templates', 'test_images', 'train.csv', 'Train_Model-Copy1.ipynb', 'Train_Model.ipynb', 'Untitled1.ipynb', 'uploads']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28039bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 42 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('my_model_without_optimizer.keras')\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f535f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd183c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06185bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e77c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdfa80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d966e947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7489e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ab93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b0f00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5889 images belonging to 5 classes.\n",
      "Found 1471 images belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1250s\u001b[0m 26s/step - accuracy: 0.1875 - loss: 1.8453 - val_accuracy: 0.1829 - val_loss: 1.8801 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1140s\u001b[0m 24s/step - accuracy: 0.2215 - loss: 1.7627 - val_accuracy: 0.1903 - val_loss: 1.8380 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1138s\u001b[0m 24s/step - accuracy: 0.2272 - loss: 1.7250 - val_accuracy: 0.2094 - val_loss: 1.7922 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1145s\u001b[0m 24s/step - accuracy: 0.2441 - loss: 1.6687 - val_accuracy: 0.2189 - val_loss: 1.7409 - learning_rate: 1.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1067s\u001b[0m 23s/step - accuracy: 0.2503 - loss: 1.6421 - val_accuracy: 0.2012 - val_loss: 1.7448 - learning_rate: 1.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1066s\u001b[0m 23s/step - accuracy: 0.2764 - loss: 1.6098 - val_accuracy: 0.2223 - val_loss: 1.7031 - learning_rate: 1.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1096s\u001b[0m 23s/step - accuracy: 0.3038 - loss: 1.5697 - val_accuracy: 0.2434 - val_loss: 1.6714 - learning_rate: 1.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1224s\u001b[0m 26s/step - accuracy: 0.3096 - loss: 1.5575 - val_accuracy: 0.2481 - val_loss: 1.6538 - learning_rate: 1.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1224s\u001b[0m 26s/step - accuracy: 0.3278 - loss: 1.5348 - val_accuracy: 0.2624 - val_loss: 1.6411 - learning_rate: 1.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1439s\u001b[0m 31s/step - accuracy: 0.3160 - loss: 1.5507 - val_accuracy: 0.2712 - val_loss: 1.6237 - learning_rate: 1.0000e-05\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 22s/step - accuracy: 0.2720 - loss: 1.6246\n",
      "Test accuracy: 0.27872195839881897\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
    "\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)\n",
    "\n",
    "def preprocess_image(image_path, input_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (input_size, input_size))\n",
    "    image = preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "image_directory = 'dataset/'\n",
    "INPUT_SIZE = 224\n",
    "BATCH_SIZE = 128  \n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2,  \n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    image_directory,\n",
    "    target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    image_directory,\n",
    "    target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "base_model = MobileNetV2(input_shape=(INPUT_SIZE, INPUT_SIZE, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  \n",
    "\n",
    "global_average_layer = layers.GlobalAveragePooling2D()\n",
    "output_layer = layers.Dense(5, activation='softmax')\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    global_average_layer,\n",
    "    output_layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('model_checkpoint.keras', save_best_only=True)\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=10,\n",
    "                    validation_data=validation_generator,\n",
    "                    callbacks=[reduce_lr, early_stop, checkpoint])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(validation_generator)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "save_directory = r'E:\\Project\\Heart attack prediction'\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "model.save(os.path.join(save_directory, 'my_model_without_optimizer.keras'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f5b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1315s\u001b[0m 17s/step - accuracy: 0.3811 - loss: 3.9637 - val_accuracy: 0.4336 - val_loss: 1.5057 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m 1/75\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17:20\u001b[0m 14s/step - accuracy: 0.4062 - loss: 1.3967"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 4s/step - accuracy: 0.4062 - loss: 1.3967 - val_accuracy: 0.4319 - val_loss: 1.5005 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1507s\u001b[0m 20s/step - accuracy: 0.4610 - loss: 1.3794 - val_accuracy: 0.4601 - val_loss: 1.5056 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 4s/step - accuracy: 0.5312 - loss: 1.4508 - val_accuracy: 0.4618 - val_loss: 1.5048 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1309s\u001b[0m 17s/step - accuracy: 0.4773 - loss: 1.3522 - val_accuracy: 0.4635 - val_loss: 1.4938 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 4s/step - accuracy: 0.4688 - loss: 1.3272 - val_accuracy: 0.4668 - val_loss: 1.4847 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1271s\u001b[0m 17s/step - accuracy: 0.4726 - loss: 1.3900 - val_accuracy: 0.4767 - val_loss: 1.4293 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 4s/step - accuracy: 0.6562 - loss: 1.4199 - val_accuracy: 0.4767 - val_loss: 1.4301 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1469s\u001b[0m 20s/step - accuracy: 0.4823 - loss: 1.3435 - val_accuracy: 0.4734 - val_loss: 1.4183 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 4s/step - accuracy: 0.5625 - loss: 1.3035 - val_accuracy: 0.4734 - val_loss: 1.4158 - learning_rate: 0.0010\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 14s/step - accuracy: 0.5251 - loss: 1.3774\n",
      "Test accuracy: 0.4953519403934479\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def load_and_preprocess_image(image_path, input_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (input_size, input_size))\n",
    "    image = image.astype('float32') / 255.0\n",
    "    return image\n",
    "\n",
    "def load_data(image_dir):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    label_map = {'No': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Proliferate': 4}\n",
    "    for label_name in label_map.keys():\n",
    "        image_paths = os.listdir(os.path.join(image_dir, label_name))\n",
    "        for image_name in image_paths:\n",
    "            if image_name.endswith('.png'):\n",
    "                image_path = os.path.join(image_dir, label_name, image_name)\n",
    "                image = load_and_preprocess_image(image_path, INPUT_SIZE)\n",
    "                dataset.append(image)\n",
    "                labels.append(label_map[label_name])\n",
    "    return np.array(dataset), np.array(labels)\n",
    "\n",
    "image_directory = 'dataset/'\n",
    "INPUT_SIZE = 224\n",
    "dataset, labels = load_data(image_directory)\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(INPUT_SIZE, INPUT_SIZE, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "predictions = layers.Dense(5, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=tf.keras.applications.vgg16.preprocess_input\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_datagen.flow(train_images, train_labels, batch_size=32),\n",
    "                    steps_per_epoch=int(len(train_images) / 32), \n",
    "                    epochs=10,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    callbacks=[reduce_lr, early_stop])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "model.save('my_model_without_optimizer.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a683bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5889 images belonging to 5 classes.\n",
      "Found 1471 images belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23s/step - accuracy: 0.2446 - loss: 2.1356 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1436s\u001b[0m 29s/step - accuracy: 0.2444 - loss: 2.1350 - val_accuracy: 0.2576 - val_loss: 1.9789 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1297s\u001b[0m 28s/step - accuracy: 0.2532 - loss: 1.9574 - val_accuracy: 0.2488 - val_loss: 1.8519 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1385s\u001b[0m 30s/step - accuracy: 0.2410 - loss: 1.8553 - val_accuracy: 0.2719 - val_loss: 1.7822 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1406s\u001b[0m 30s/step - accuracy: 0.2541 - loss: 1.7580 - val_accuracy: 0.2631 - val_loss: 1.6879 - learning_rate: 1.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1878s\u001b[0m 40s/step - accuracy: 0.2494 - loss: 1.7196 - val_accuracy: 0.2848 - val_loss: 1.6434 - learning_rate: 1.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1818s\u001b[0m 39s/step - accuracy: 0.2659 - loss: 1.6782 - val_accuracy: 0.2821 - val_loss: 1.6202 - learning_rate: 1.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1481s\u001b[0m 31s/step - accuracy: 0.2849 - loss: 1.6312 - val_accuracy: 0.2835 - val_loss: 1.6051 - learning_rate: 1.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1075s\u001b[0m 23s/step - accuracy: 0.2803 - loss: 1.6112 - val_accuracy: 0.2957 - val_loss: 1.5727 - learning_rate: 1.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1044s\u001b[0m 22s/step - accuracy: 0.2986 - loss: 1.5746 - val_accuracy: 0.3134 - val_loss: 1.5472 - learning_rate: 1.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1030s\u001b[0m 22s/step - accuracy: 0.3006 - loss: 1.5694 - val_accuracy: 0.3297 - val_loss: 1.5160 - learning_rate: 1.0000e-05\n",
      "Epoch 1/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9026s\u001b[0m 193s/step - accuracy: 0.3006 - loss: 1.5182 - val_accuracy: 0.3372 - val_loss: 1.5070 - learning_rate: 1.0000e-06\n",
      "Epoch 2/10\n",
      "\u001b[1m 2/47\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:27:56\u001b[0m 197s/step - accuracy: 0.3438 - loss: 1.4697"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
    "\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)\n",
    "\n",
    "image_directory = 'dataset/'\n",
    "INPUT_SIZE = 224\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    image_directory,\n",
    "    target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    image_directory,\n",
    "    target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "base_model = MobileNetV2(input_shape=(INPUT_SIZE, INPUT_SIZE, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False \n",
    "\n",
    "global_average_layer = layers.GlobalAveragePooling2D()\n",
    "output_layer = layers.Dense(5, activation='softmax')\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    global_average_layer,\n",
    "    output_layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('model_checkpoint.keras', save_best_only=True)\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=10,\n",
    "                    validation_data=validation_generator,\n",
    "                    callbacks=[reduce_lr, early_stop, checkpoint])\n",
    "\n",
    "for layer in base_model.layers[:100]:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[100:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_fine = model.fit(train_generator,\n",
    "                         epochs=10,\n",
    "                         validation_data=validation_generator,\n",
    "                         callbacks=[reduce_lr, early_stop, checkpoint])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(validation_generator)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "save_directory = r'E:\\Project\\Heart attack prediction'\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "model.save(os.path.join(save_directory, 'my_model_without_optimizer.keras'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62c4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5889 images belonging to 5 classes.\n",
      "Found 1471 images belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19s/step - accuracy: 0.1869 - loss: 1.8625 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1180s\u001b[0m 25s/step - accuracy: 0.1869 - loss: 1.8620 - val_accuracy: 0.1883 - val_loss: 1.7986 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1167s\u001b[0m 25s/step - accuracy: 0.2051 - loss: 1.7672 - val_accuracy: 0.2379 - val_loss: 1.7156 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1171s\u001b[0m 25s/step - accuracy: 0.2500 - loss: 1.6878 - val_accuracy: 0.2746 - val_loss: 1.6540 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1172s\u001b[0m 25s/step - accuracy: 0.2648 - loss: 1.6253 - val_accuracy: 0.2896 - val_loss: 1.6109 - learning_rate: 1.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1167s\u001b[0m 25s/step - accuracy: 0.2868 - loss: 1.5928 - val_accuracy: 0.3215 - val_loss: 1.5798 - learning_rate: 1.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1170s\u001b[0m 25s/step - accuracy: 0.2976 - loss: 1.5691 - val_accuracy: 0.3270 - val_loss: 1.5685 - learning_rate: 1.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1160s\u001b[0m 25s/step - accuracy: 0.3200 - loss: 1.5446 - val_accuracy: 0.3209 - val_loss: 1.5413 - learning_rate: 1.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1216s\u001b[0m 26s/step - accuracy: 0.3291 - loss: 1.5287 - val_accuracy: 0.3372 - val_loss: 1.5222 - learning_rate: 1.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1077s\u001b[0m 23s/step - accuracy: 0.3451 - loss: 1.5033 - val_accuracy: 0.3841 - val_loss: 1.4766 - learning_rate: 1.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1042s\u001b[0m 22s/step - accuracy: 0.3611 - loss: 1.4824 - val_accuracy: 0.3623 - val_loss: 1.4913 - learning_rate: 1.0000e-05\n",
      "Epoch 1/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5715s\u001b[0m 121s/step - accuracy: 0.2531 - loss: 1.6193 - val_accuracy: 0.4004 - val_loss: 1.4466 - learning_rate: 1.0000e-06\n",
      "Epoch 2/10\n",
      "\u001b[1m17/47\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07:18\u001b[0m 135s/step - accuracy: 0.2840 - loss: 1.5713"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
    "\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)\n",
    "\n",
    "image_directory = 'dataset/'\n",
    "INPUT_SIZE = 224\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2, \n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    image_directory,\n",
    "    target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    image_directory,\n",
    "    target_size=(INPUT_SIZE, INPUT_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "base_model = MobileNetV2(input_shape=(INPUT_SIZE, INPUT_SIZE, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  \n",
    "\n",
    "global_average_layer = layers.GlobalAveragePooling2D()\n",
    "output_layer = layers.Dense(5, activation='softmax')\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    global_average_layer,\n",
    "    output_layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('model_checkpoint.keras', save_best_only=True)\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=10,\n",
    "                    validation_data=validation_generator,\n",
    "                    callbacks=[reduce_lr, early_stop, checkpoint])\n",
    "\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:100]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_fine = model.fit(train_generator,\n",
    "                         epochs=10,\n",
    "                         validation_data=validation_generator,\n",
    "                         callbacks=[reduce_lr, early_stop, checkpoint])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(validation_generator)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "save_directory = r'E:\\Project\\Heart attack prediction'\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "model.save(os.path.join(save_directory, 'my_model_without_optimizer.keras'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1180d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.densenet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc2e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, input_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (input_size, input_size))\n",
    "    image = preprocess_input(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39f4d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(image_dir):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    label_map = {'No': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Proliferate': 4}\n",
    "    for label_name in label_map.keys():\n",
    "        image_paths = os.listdir(os.path.join(image_dir, label_name))\n",
    "        for image_name in image_paths:\n",
    "            if image_name.endswith('.png'):\n",
    "                image_path = os.path.join(image_dir, label_name, image_name)\n",
    "                image = load_and_preprocess_image(image_path, INPUT_SIZE)\n",
    "                dataset.append(image)\n",
    "                labels.append(label_map[label_name])\n",
    "    return np.array(dataset), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeb71233",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "670bf198",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5952f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, labels = load_data(image_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a31d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "165406ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = DenseNet121(input_shape=(INPUT_SIZE, INPUT_SIZE, 3), include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d6b012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e71a52d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7c67f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = layers.GlobalAveragePooling2D()\n",
    "dropout = layers.Dropout(0.5)  \n",
    "output_layer = layers.Dense(5, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e673cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  dropout,\n",
    "  output_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffd1256e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument(s) not recognized: {'lr': 1e-05}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m),\n\u001b[0;32m      2\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\adam.py:62\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     45\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     61\u001b[0m ):\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     63\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m     64\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m     65\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m     66\u001b[0m         clipnorm\u001b[38;5;241m=\u001b[39mclipnorm,\n\u001b[0;32m     67\u001b[0m         clipvalue\u001b[38;5;241m=\u001b[39mclipvalue,\n\u001b[0;32m     68\u001b[0m         global_clipnorm\u001b[38;5;241m=\u001b[39mglobal_clipnorm,\n\u001b[0;32m     69\u001b[0m         use_ema\u001b[38;5;241m=\u001b[39muse_ema,\n\u001b[0;32m     70\u001b[0m         ema_momentum\u001b[38;5;241m=\u001b[39mema_momentum,\n\u001b[0;32m     71\u001b[0m         ema_overwrite_frequency\u001b[38;5;241m=\u001b[39mema_overwrite_frequency,\n\u001b[0;32m     72\u001b[0m         loss_scale_factor\u001b[38;5;241m=\u001b[39mloss_scale_factor,\n\u001b[0;32m     73\u001b[0m         gradient_accumulation_steps\u001b[38;5;241m=\u001b[39mgradient_accumulation_steps,\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     75\u001b[0m     )\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1 \u001b[38;5;241m=\u001b[39m beta_1\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2 \u001b[38;5;241m=\u001b[39m beta_2\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:21\u001b[0m, in \u001b[0;36mTFOptimizer.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:90\u001b[0m, in \u001b[0;36mBaseOptimizer.__init__\u001b[1;34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `decay` is no longer supported and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m     )\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument(s) not recognized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     name \u001b[38;5;241m=\u001b[39m auto_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Argument(s) not recognized: {'lr': 1e-05}"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizers.Adam(lr=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf2d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                    epochs=15,  \n",
    "                    batch_size=32,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e72dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07890536",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_without_optimizer.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c81e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "\n",
    "def load_and_preprocess_image(image_path, input_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (input_size, input_size))\n",
    "    image = preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "def load_data(image_dir):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    label_map = {'No': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Proliferate': 4}\n",
    "    for label_name in label_map.keys():\n",
    "        image_paths = os.listdir(os.path.join(image_dir, label_name))\n",
    "        for image_name in image_paths:\n",
    "            if image_name.endswith('.png'):\n",
    "                image_path = os.path.join(image_dir, label_name, image_name)\n",
    "                image = load_and_preprocess_image(image_path, INPUT_SIZE)\n",
    "                dataset.append(image)\n",
    "                labels.append(label_map[label_name])\n",
    "    return np.array(dataset), np.array(labels)\n",
    "\n",
    "image_directory = 'dataset/'\n",
    "INPUT_SIZE = 224\n",
    "dataset, labels = load_data(image_directory)\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "base_model = DenseNet121(input_shape=(INPUT_SIZE, INPUT_SIZE, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "global_average_layer = layers.GlobalAveragePooling2D()\n",
    "dropout = layers.Dropout(0.5)  \n",
    "output_layer = layers.Dense(5, activation='softmax')\n",
    "\n",
    "model = models.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  dropout,\n",
    "  output_layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-5), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_datagen.flow(train_images, train_labels, batch_size=32),\n",
    "                    steps_per_epoch=len(train_images) / 32,\n",
    "                    epochs=30,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "model.save('heart_attack_prediction_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97788307",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_without_optimizer.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15e782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
